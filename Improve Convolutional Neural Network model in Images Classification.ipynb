{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Improve Convolutional Neural Network model in Images Classification</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "0. [Title and author](#Title-and-author:)\n",
    "\n",
    "\n",
    "1. [Introduction](#Introduction:)\n",
    "\n",
    "\n",
    "2. [Dataset](#Dataset:)\n",
    "\n",
    "\n",
    "3. [Methodology](#Methodology:)\n",
    "\n",
    "\n",
    "4. [Results](#Results:)\n",
    "\n",
    "\n",
    "5. [References](#References:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and author:\n",
    "1. <b>Title</b>:\n",
    "<center><i>Improve Convolutional Neural Network model in Images Classification</i></center>\n",
    "\n",
    "2. <b>Author</b>:\n",
    "    - Name: Thai Quoc Hoang\n",
    "    - Date of birth: January 09, 2000\n",
    "    - Email: quocthai9120@gmail.com / qthai912@uw.edu\n",
    "    - GitHub: https://github.com/quocthai9120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "This project is done based on the result from the final research project in the CSE163 course from the University of Washington about Using different models in Image Classification. The link below gives more information about that research project: https://github.com/quocthai9120/Images-Classification.\n",
    "\n",
    "The previous result ends up with 70.09% correct accuracy for 10000 testing instances using the CIFAR_10 dataset. This is not a bad start for the image classification task. However, we need to improve the model significantly if we want to use in real-life classification tasks.\n",
    "\n",
    "This project focuses on experimenting with different methods to improve the Convolutional Neural Network model that used to image classification using the CIFAR_10 dataset, including data preprocessing, pre-trained model modification, and training modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "\n",
    "\n",
    "## CIFAR_10 dataset:\n",
    "\n",
    "### Source and Information:\n",
    "The CIFAR-10 dataset contains 60000 32x32 colour images in 10 classes (6000 images/class) that are randomly shuffled and are divided into 2 parts:\n",
    "- 50000 images for training (5000 images for each class).\n",
    "- 10000 images for testing (1000 images for each class).\n",
    "\n",
    "The classes are completely mutually exclusive and there is no overlap between classes.\n",
    "\n",
    "10 classes of CIFAR-10 dataset are:\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog\n",
    "6. frog\n",
    "7. horse\n",
    "8. ship\n",
    "9. truck\n",
    "\n",
    "The dataset were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton and can be accessed through the link provided here : https://www.cs.toronto.edu/~kriz/cifar.html or directly download here: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz.\n",
    "\n",
    "### Dataset layout:\n",
    "The CIFAR-10 datset used in this project is the Python Version.\n",
    "\n",
    "The download compressed file contains 7 files: data_batch_1, data_batch_2, data_batch_4, data_batch_5,test_batch, and batches.meta, which are Python \"pickled\" objects.\n",
    "\n",
    "The source of the dataset (https://www.cs.toronto.edu/~kriz/cifar.html) provides a function called 'unpickle(file)' to open such files and returns the data as dictionaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology:\n",
    "\n",
    "To improve the CNN model in Images Classification task, we experiment with multiple methods for the model.\n",
    "\n",
    "## Data Augmentation:\n",
    "\n",
    "A fundamental principle of Deep Learning is that the quantity and quality of data plays an important role in determining how well the model behaves. However, collecting new data requires a trade of money and time. With that reason, data augmentation is an appropriated solution to generate more data using the existing data.\n",
    "\n",
    "For this project, as CIFAR_10 is a complex dataset in which images have different shapes, colors, backgrounds,... we will generate new data by augmenting existing data while training the model by each batch rather than generate images before training to utilize more augmenting data and also avoid overfitting during the training process.\n",
    "\n",
    "Several examples of augmented images generated while training the model: <img src=\"Supporting Files/Improved Results/Augmented Images.png\" style=\"width: 450px\"/>\n",
    "\n",
    "\n",
    "## Data Normalization:\n",
    "\n",
    "The deeper network requires longer training time. To decrease the training time, we need to make convergence faster while training the model. Therefore, data normalization plays an important role in contributing to this process.\n",
    "\n",
    "For this project, we normalization the data using the formula:\n",
    "\n",
    "<center>$X_n = {(X - \\mathbf{μ})/\\mathbf{σ}}$, where:</center>\n",
    "\n",
    "- $X_n$: The normalized image\n",
    "- $X$: The original image\n",
    "- $μ$: The mean of values of the image\n",
    "- $σ$: The standard deviation of values of the image\n",
    "\n",
    "The algorithm is applied in two steps:\n",
    "- Subtracting the mean from each pixel\n",
    "- Dividing each pixel by the standard deviation\n",
    "\n",
    "\n",
    "## Batch Normalization:\n",
    "\n",
    "During the training process, the distribution of activations is continuously changing after each step of backpropagation, which causes the training process slower. Batch Normalization allows us to normalize the distribution of activations by adjusting and scaling the activations so that the input for every layer will have a similar distribution after each training step.\n",
    "\n",
    "For this project, we set up the batch normalization after using ReLU activation function for each Convolutional layer and for Dense layer.\n",
    "\n",
    "\n",
    "## Modify the Neural Network model:\n",
    "\n",
    "The previous model resulted in 70.09% accuracy after training 25 epochs. If we continue training the model, we will have an overfitting problem. To overcome this overfitting and improve the accuracy, we need to modify the whole model by tuning the hyperparameters and adding layers to the model.\n",
    "\n",
    "According to the paper of Reconciling modern machine learning and the bias-variance\n",
    "trade-off, as described in [5], the deep learning model could have the \"Double descent\" risk curve. That is if we make the model complex enough, the loss will start decreasing again.\n",
    "\n",
    "We need to add more convolutional layers to learn more abstract features of the data. To do so, we also need to decrease the complexity of the dense layer.\n",
    "\n",
    "In addition, as we are using Batch Normalization during the training process, it is necessary to decrease the Dropout rate to avoid confliction with batch normalization.\n",
    "\n",
    "We use the loss function categorical crossentropy:\n",
    "\n",
    "<center>Cross Entroy Loss = $-\\sum_{i}^{C} {t_i log(s_i)}$, where:</center>\n",
    "\n",
    "Summary of the improved model: <img src=\"Supporting Files/Improved Results/Models comparison.png\" style=\"width: 450px\"/>\n",
    "\n",
    "## Reduce Learning Rate:\n",
    "\n",
    "In addition, we also need to prevent the case that the high learning rate causes the model hard to get converge or even get diverge.\n",
    "\n",
    "For our current model, we use a small modification by reducing the learning rate using the formula:\n",
    "<center>$L_n = L * ⍺$, where:</center>\n",
    "\n",
    "- $L_n$: new learning rate\n",
    "- $L$: current learning rate\n",
    "- $⍺$: multiplication factor\n",
    "\n",
    "We set up the patience for the model to decrease the learning rate if the model does not decrease loss for 3 training epochs.\n",
    "\n",
    "## Early Stopping:\n",
    "\n",
    "Because we are experimenting with different methods, we need to explore a huge range of training epochs. Early stopping allows us to stop when the model converges or starts to get diverge so that we can explore training the model with more epochs.\n",
    "\n",
    "For our current model, we set up the patience for the model to stop training if the model does not decrease loss for 10 training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "- The project has a 86.25% correct accuracy using 10000 testing instances for classification, which is increased 16.16% from 70.09% as the previous project. Using several images from testing set to compare two models: <img src=\"Supporting Files/Improved Results/CNN predictions.png\" style=\"width: 1000px\"/>\n",
    "\n",
    "- Using the first ten examples in test set and compute the probabilities for each label, we got the results for the previous model and the improved model:\n",
    "    + Previous model: <img src=\"cifar-10-batches-py/Images/CNN predictions probabilities.jpg\" style=\"width: 1000px\"/>\n",
    "    + Improved model: <img src=\"Supporting Files/Improved Results/CNN predictions probabilities.png\" style=\"width: 1000px\"/>\n",
    "\n",
    "    + As the 10 examples are shuffled randomly, the results can represent the trend of the CIFAR_10 dataset. From the results we have produced, we recognize the fact that for most images in this dataset, the improved CNN model gives predictions with higher confidence because along with the high value of resulted label's probabilities, the improved model reduces the probabilities of wrong predictions (the values of other labels) that we can refer to the loss of the prediction.\n",
    "\n",
    "- The improved model also decreases loss smoother compared to the previous model and does not have the overfitting problem at the last few epochs like the previous model: <img src=\"Supporting Files/Improved Results/CNN Val loss history comparison.png\" style=\"width: 1000px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]. CS231n Convolutional Neural Networks for Visual Recognition - Data Preprocessing: http://cs231n.github.io/neural-networks-2/#datapre\n",
    "\n",
    "[2]. Jason Brownlee. A Gentle Introduction to Early Stopping to Avoid Overtraining Deep Learning Neural Network Models: https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/\n",
    "\n",
    "[3]. Sergey Ioffe, Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift: https://arxiv.org/pdf/1502.03167v3.pdf\n",
    "\n",
    "[4]. Francois Chollet. Building powerful image classification models using very little data: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "[5]. Mikhail Belkin, Daniel Hsu, Siyuan Ma , and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off: https://arxiv.org/pdf/1812.11118.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
